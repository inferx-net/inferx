{
    "type": "function",
    "tenant": "t1",
    "namespace": "ns1",
    "name": "llama1",
    "spec": {
        "image": "vllm/vllm-openai:v0.6.2",
        "commands": [
            "--model",
            "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
            "--enforce-eager"
        ],
        "resources": {
            "CPU": 100,
            "Mem": 200,
            "GPU": {
                "Type": "Any",
                "Usage": {
                    "Partial": 50
                }
            }
        },
        "envs": [
            [
                "LD_LIBRARY_PATH",
                "/usr/local/lib/python3.12/dist-packages/nvidia/cuda_nvrtc/lib/:$LD_LIBRARY_PATH"
            ]
        ],
        "mounts": [
            {
                "hostpath": "/home/brad/cache",
                "mountpath": "/root/.cache/huggingface"
            }
        ],
        "endpoint": {
            "path": "/v1/completions",
            "port": 8000,
            "schema": "Http"
        },
        "probe": {
            "path": "/health",
            "port": 8000,
            "schema": "Http"
        },
        "api_type": {
            "openai": {
                "name": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
                "max_tokens": 200,
                "temperature": 0
            }
        }
    }
}